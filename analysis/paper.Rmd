---
title: "DATA 598: Replication Project"
output: bookdown::word_document2
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
For examing the complexities in replication we chose the paper [@huffer_graham_2017]. This paper is a study of the online trade of human remains on Instagram. The paper focuses on understanding what is happening, where it is happening, and how these human remains are framed as collectible objects so that archaeologists, cultural heritage professionals, museums and so on are better equipped to engage with this desire and to channel it productively. The paper says that due to the accessibility of social media for the use of selling, displaying and trading human remains is so feasible, it has led to treating human remains as consumer products for the collector’s markets rather than objects of archaeological, ethnographic or anatomical value which is a cause of great concern. Many state or national-level jurisdictions have introduced specific restrictions related to the sale and transport of human remains, above and beyond the requirements laid upon signatories to UNESCO (1970) cultural heritage conventions, which applies to most source and demand countries. The study in the paper is to better help these organizations understand the dynamics of human remains trade so that they can take informed actions and make effective policies.

The authors of the paper use several methods employing data mining techniques to query the "noise" surrounding the topics related to the trade of human remains on Instagram. The paper demonstrates that these methods show great promise as opposed to time and labor-intensive manual search methods.

We have chosen to replicate one such method in this project which involves topic modeling. The purpose of this technique is to extract semantic structure from posts. These topics can be used to classify and group different posts relating to trade. This creates a foundation to perform much deeper research and analysis into understanding the different trends and patterns associated with different aspects of the trade such as buying and selling, sales prices, the material being traded and other mechanics of trade. The methods used in the research take advantage of the open data as the images, captions, location, user id, the selling price, etc. mentioned in the post to study the characteristics of the online trade. Although they have removed the usernames from the published data and figures but have used them in the analysis.

# Replication Source

From the paper [@huffer_graham_2017], we are replicating Figure \@ref(fig:original-fig) (Figure 5 in the paper). This figure shows different topics modeled from the posts from the account of a single trader/user on Instagram. The key conclusions from this figure are that this particular trader is interested in "real" or authentic human bone artifacts. The original method used by the authors of the paper for the topic modeling is through the use of the ‘mallet’ package in R. We have used ‘gensim’ package in python for the same. We make this change of library so as to replicate the study, as this is like changing the tools/environment in which analysis is done but it should not change the results.

```{r, original-fig, fig.cap="Original figure from [@huffer_graham_2017]", echo=FALSE}
knitr::include_graphics("figures/fig5-singletrader.png")
```

# Replication Method
Figure 5: Topics within the posts of a single Instagram account, is related to topic modeling and was originally carried out using the ‘mallet’ package in R. We decided to use the ‘gensim’ package in Python to perform topic modeling and replicate the figure. 
Steps:
 1. We read in the csv file and removed stopwords as specified in the paper. 
 2. We converted texts from each of the instagram posts into a vector of words and stored it in a variable called ‘data_words’. 
 3. We created a variable called ‘corpus’ that maps every word to an id and calculates the term document frequency.
 4. We built an LDA model using the ‘gensim’ package and specified the parameter for the number of topics as 25. This model is named ‘lda’. 
 5. Wrote down our results in preliminary CSV. Then further we read the CSV with our R chunk and we were able to approximate the visualisation in R. 
 6. We also displayed all the words relating to a particular topic in R. 
 
 We kept the same criteria for stopwords and the number of topics as the original study. We followed the same analytical procedure. The ‘gensim’ package in Python is for topic modeling and has similar functions to the ‘mallet’ package used by the original study. Since we use the same dataset and methods and only change the operating system, the result should be the same. 


```{r cache=TRUE, include=FALSE}
#install.packages("reticulate",repos = "http://cran.us.r-project.org")
library(reticulate)
use_virtualenv("r-reticulate")
#virtualenv_install("r-reticulate", ignore_installed = FALSE)
py_available(TRUE)
py_install("pandas", pip = TRUE)
py_install("nltk", pip = TRUE)
py_install("gensim", pip = TRUE)
py_install("numpy", pip = TRUE)
```

```{python include=FALSE}
import nltk
nltk.download('stopwords')
import re
import pandas as pd
from pprint import pprint
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel
import numpy 
#import pyLDAvis
#import pyLDAvis.gensim
#import matplotlib.pyplot as plt
#import logging
#import warnings

```

```{python}
stop_words = pd.read_csv('..\data\en.txt',header=None)
stop_words=stop_words[0].to_list()
df = pd.read_csv('..\data\posts-formatted-for-topicmodelling.csv', encoding='latin-1')
df.head()
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence),deacc=True)) #deacc = True removes punctuation

data_words = list(sent_to_words(df.text))
#print(data_words)
#print([word  for word in simple_preprocess(str(data_words[0])) if word not in stop_words])
#print( type(simple_preprocess(str(data_words[0]))))
def remove_stopwords(texts):
    return[[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
data_words_nostops = remove_stopwords(data_words)
#print(data_words_nostops)

```

```{python include=FALSE}
id2word = corpora.Dictionary(data_words_nostops)
#create Corpus
texts = data_words_nostops
```


```{python include=FALSE}
#term document frequency
corpus = [id2word.doc2bow(text) for text in texts]
#print(corpus)

lda = gensim.models.ldamodel.LdaModel(corpus, num_topics=25,id2word=id2word,minimum_probability =0, random_state=100)
x=lda.show_topics(num_topics=25,num_words=3,formatted=False)
topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]
l=[]
for topic,words in topics_words:
    l.append(".".join(words))

```

```{python include=FALSE}
y=lda.show_topics(num_topics=25,formatted=False)
topics_words2 = [(tp[0], [wd[0] for wd in tp[1]]) for tp in y]
l2=[]
for topic,words in topics_words2:
    l2.append(".".join(words))
```


```{python include=FALSE}
doct=lda.get_document_topics(corpus,minimum_probability =0.0)
df=pd.DataFrame([[x[1] for x in y] for y in doct], index = [x for x in range(len(doct))])
df.columns = l
df.to_csv("D:\MSDS\Winter 2020\Reproducibility for DS\Replication-Project-DATA598\data\prob.csv")
```


```{r echo=FALSE}
prob_matrix <- read.csv("../data/prob.csv", header=TRUE)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
require(reshape2)
plot_figure <- function() {
 topic_docs <- prob_matrix
  captionstext <- read.csv('../data/posts-formatted-for-topicmodelling.csv', stringsAsFactors = FALSE)
## kludge for when username comesthrough as numeric rather than character
  captionstext$username <-as.character(captionstext$username)

  documents <- data.frame(text = captionstext$text,
                        id = make.unique(captionstext$username),
                        class = captionstext$year, 
                        stringsAsFactors=FALSE)
  topic_docs<- t(topic_docs[,-1])
  names(topic_docs) <- documents$id
# find top n topics for a certain author
  df1 <- t(topic_docs[,grep("234396855", names(topic_docs))])
  #8963295 is a person who has 'for sale' in her post
  #255766488 natural_selections - skullshop.ca
  #361451583 ryan matthew cohn
  #234396855 pandora's box, York
  topic.proportions.df <- melt(cbind(data.frame(df1),
                                   document=factor(1:nrow(df1))),
                             variable.name="topic",
                             id.vars = "document") 
  
  # plot for each doc by that author
  require(ggplot2)
  dpi=600    #pixels per square inch
  png("../analysis/figures/output_figure.png", width=14*dpi, height=14*dpi, res=dpi)

  print({
    p <- ggplot(topic.proportions.df, aes(topic, value, fill=document)) +
      geom_bar(stat="identity") +
      ylab("proportion") +
      theme(axis.text.x = element_text(angle=90, hjust=1)) +  
      coord_flip() +
      facet_wrap(~ document, ncol=5)
  })
  dev.off()
}
```

# Replicated Figures

```{r warning=FALSE, message=FALSE, include=FALSE, echo=TRUE}
plot_figure()
```

```{r echo=FALSE}
knitr::include_graphics("../analysis/figures/output_figure.png")
```

# Conclusion


# References
